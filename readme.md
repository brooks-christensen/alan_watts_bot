# Alan Watts Chatbot Release Notes

## THIS PROJECT IS STILL UNDER ACTIVE DEVELOPMENT
The development has primarily been conducted in notebooks to-date, although an initial automated training pipeline was created.

Since a dialogic dataset is required to interact with the language model as intended, the training pipeline needs to be redesigned to accommodate from the current monologic generation style that is currently in place.

## Project Objectives
After listening to some great lectures from the late Philosopher Alan Watts, I was inspired to create a chatbot that can give answers to life's great questions in his unique style.

The chatbot should be able to 
1. Take text input (possible upgrade: voice input) in the form of a question
2. Query an encoder-decoder transformer model to generate a text response
3. Generate voice audio from the text response

## Project Tasks
1) Groom Data
    - Large Language Model (LLM)
    - Q&A Model
    - Text-to-Speech (T2S) Model

2) Train Models
    - Fine-tune pretrained GPT2 model on LLM groomed data
    - Fine-tune LLM model on Q&A model groomed data
    - Fine-tune T2S model

## Data Preparation Notes
[Data Source](https://archive.org/details/alanwattscollection)

### Data Grooming for the Large Language Model (LLM)
1) Create text file from OpenAI Speech-to-Text (S2T) model Whisper
2) Remove all text that was not generated by the voice of Alan Watts himself
3) Remove duplicate or repetative paragraphs and rows
4) 

### Data Grooming for the Q&A Model
1) 

### Data Grooming for the T2S Model
1) Use text data from LLM data grooming to compare when Alan Watts was not speaking
2) Cut out audio from audio files in the data source when Alan Watts was not speaking
3) 

## Model Training Notes

### Fine-Tuning GPT2 Model on LLM Data
- decoder-only model
- process overview:
    - load input
    - tokenize input
    - create dataloaders
    - load GPT2
    - create optimizer
    - create LR scheduler
    - train
    - evaluation / reporting
    - generate text
- next steps:
    - understand how hyperparameters from [Andrej Karpathy's video](https://www.youtube.com/watch?v=kCc8FmEb1nY) will need to be adjusted to compensate for the increased number of parameters in the model compared to the Shakespeare generator model
    - Ensure the data is being loaded properly into the pretrained ChatGPT2 model
        - dimensions
        - size
        - number